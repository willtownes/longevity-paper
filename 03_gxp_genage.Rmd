---
title: "Predicting Pro vs Anti Longevity from Gene Expression"
author: "Will Townes"
date: "3/29/2018"
output: html_document
---

Using gene expression alone to predict pro vs anti longevity genes from genage database.

```{r}
library(class)
library(caret)
library(glmnet)
library(xgboost)
library(ROCR)
spp<-c("yeast","worm")
sp<-spp[2]
features<-c("archs4","other1")
fts<-features[1]
```

## Data Loading

```{r}
dat<-readRDS(paste0("./data/auto/",sp,".rds"))
y<-rep("anti",length(dat$y))
y[dat$y==1]<-"pro"
y<-factor(y)
#ytrn<-dat$y[dat$train_idx]
#ytst<-dat$y[-dat$train_idx]
ytrn<-y[dat$train_idx]
ytst<-y[-dat$train_idx]
X<-dat[[fts]]
Xtrn<-X[dat$train_idx,]
Xtst<-X[-dat$train_idx,]
```

knn classifier

```{r}
fit<-knn(Xtrn,Xtst,ytrn,k=10,prob=TRUE)
probs<-attr(fit,"prob")
probs[fit=="anti"]<-1-probs[fit=="anti"]
preds<-prediction(probs,ytst)
auc.tmp<-performance(preds,"auc")
(auc<-as.numeric(auc.tmp@y.values)) #0.6

#knn via caret
system.time(fit<-train(Xtrn, ytrn, preProcess=NULL, distribution="bernoulli", verbose=FALSE, metric="Kappa", trControl=trainControl("repeatedcv",5,repeats=2),  method="kknn", tuneGrid=expand.grid(kmax=c(5,10,15),distance=c(1,2),kernel="optimal")))
#classProbs=TRUE)
print(fit)
p1<-prediction(predict(fit,type="prob")[,2], ytrn)
p2<-prediction(predict(fit,Xtst,type="prob")[,2], ytst)
(auc<-performance(p2,"auc")@y.values[[1]])
plot(performance(p1,"tpr","fpr"),lty=2)
plot(performance(p2,"tpr","fpr"),add=TRUE)
abline(0,1,lty=3)
```

xgboost

```{r}
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
  nrounds = 50,
  max_depth = c(2, 4),
  eta = c(0.1, 0.01),
  gamma = 1,
  colsample_bytree=1,
  min_child_weight=1,
  subsample=1
)

# pack the training control parameters
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 3,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",                                     
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  allowParallel = TRUE
)

fit = train(
  Xtrn,
  y = factor(paste0("pro",ytrn)),
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree",
  metric="Kappa"
)


#xgb_pars<-list(objective="binary:logistic",eta=.3,max_depth=6,nthread=2)
#fit<-xgb.cv(xgb_pars, Xtrn, nrounds=10, nfold=5, label=ytrn)
probs<-predict(fit,Xtst,type="prob")[,"pro1"]
preds<-prediction(probs,ytst)
auc.tmp<-performance(preds,"auc")
(auc<-as.numeric(auc.tmp@y.values)) #0.6
```

gradient boosted machines

```{r}
system.time(res2<-train(Xtrn,factor(ytrn), method="gbm", distribution="bernoulli", preProcess=NULL, verbose=FALSE, metric="Kappa", trControl=trainControl("cv",5), tuneGrid=expand.grid(interaction.depth=c(1,2),n.trees=c(800,1600),shrinkage=.005,n.minobsinnode=10)))
print(res2)
probs<-predict(res2,newdata=Xtst,type="prob")[,"1"]
preds<-prediction(probs,ytst)
auc.tmp<-performance(preds,"auc")
(auc<-as.numeric(auc.tmp@y.values)) #0.6
```

svm with caret
```{r}
system.time(res3<-train(Xtrn,factor(ytrn), method="svmRadial", distribution="bernoulli", preProcess=NULL, verbose=FALSE, metric="Kappa", trControl=trainControl("cv",5),prob.model=TRUE))
print(res3)
probs<-predict(res3,newdata=Xtst,type="prob")[,"1"]
preds<-prediction(probs,ytst)
auc.tmp<-performance(preds,"auc")
(auc<-as.numeric(auc.tmp@y.values)) #0.6
```

GLMnet prediction

```{r}
pp<-preProcess(Xtrn,method=c("center","scale"))
Xtrn<-cbind(1,predict(pp,Xtrn))
Xtst<-cbind(1,predict(pp,Xtst))

#folds<-createFolds(ytrn)
#res<-train(mean_lifespan~.,data=d[train_idx,],preProcess=c("center","scale"))

alpha<-if(pca){ 0.0 }else{ .9 }
res<-cv.glmnet(Xtrn,ytrn,family="binomial",alpha=alpha)
probs<-predict(res, newx=Xtst, s="lambda.min", type="response")
#saveRDS(res,file="glmnet_fit.rds")
plot(res)
preds<-prediction(probs,ytst)
auc.tmp<-performance(preds,"auc")
(auc<-as.numeric(auc.tmp@y.values)) #0.5
```

```{r}
rmse<-function(ytrue,ypred){
  sqrt(mean((ytrue-ypred)^2))
}
(rmse_glmnet<-rmse(probs,ytst))
(rmse_mean_only<-rmse(mean(ytrn),ytst))
```

